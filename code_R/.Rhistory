multilevel_generic_posterior_pred <- pp_check(m_multilevel_generic_fit,
nsamples = 100) +
labs(title = "R/brms: posterior predictive check")
save_plot(path = "../plots_R/multilevel_generic_posterior_pred.png")
# weak model
multilevel_weak_posterior_pred <- pp_check(m_multilevel_weak_fit,
nsamples = 100) +
labs(title = "R/brms: posterior predictive check")
save_plot(path = "../plots_R/multilevel_weak_posterior_pred.png")
# Chunk 11
# specific
fixed_interval_groups(fit = m_multilevel_specific_fit,
title = "Fixed effect interval (.95, .8)",
data = train,
n_time = 100)
save_plot(path = "../plots_R/multilevel_specific_HDI_fixed.png")
# generic
fixed_interval_groups(fit = m_multilevel_generic_fit,
title = "Fixed effect interval (.95, .8)",
data = train,
n_time = 100)
save_plot(path = "../plots_R/multilevel_generic_HDI_fixed.png")
# weak
fixed_interval_groups(fit = m_multilevel_weak_fit,
title = "Fixed effect interval (.95, .8)",
data = train,
n_time = 100)
save_plot(path = "../plots_R/multilevel_weak_HDI_fixed.png")
# Chunk 12
# specific
prediction_interval_groups(fit = m_multilevel_specific_fit,
title = "Prediction interval (.95, .8)",
data = train,
n_time = 100)
save_plot(path = "../plots_R/multilevel_specific_HDI_full.png")
# generic
prediction_interval_groups(fit = m_multilevel_generic_fit,
title = "Prediction interval (.95, .8)",
data = train,
n_time = 100)
save_plot(path = "../plots_R/multilevel_generic_HDI_full.png")
# weak
prediction_interval_groups(fit = m_multilevel_weak_fit,
title = "Prediction interval (.95, .8)",
data = train,
n_time = 100)
save_plot(path = "../plots_R/multilevel_weak_HDI_full.png")
# Chunk 13
# for consistency with python.
# does not do anything at the moment..
width = 7
height = 4
# specific
mcmc_hdi(fit = m_multilevel_specific_fit,
title = "R/brms: HDI intervals for parameters")
save_plot(path = "../plots_R/multilevel_specific_HDI_param.png",
width = width,
height = height)
# generic
mcmc_hdi(fit = m_multilevel_generic_fit,
title = "R/brms: HDI intervals for parameters")
save_plot(path = "../plots_R/multilevel_generic_HDI_param.png",
width = width,
height = height)
# weak
mcmc_hdi(fit = m_multilevel_weak_fit,
title = "R/brms: HDI intervals for parameters")
save_plot(path = "../plots_R/multilevel_weak_HDI_param.png",
width = width,
height = height)
# fit the first model
get_prior(formula = f_multilevel,
data = train,
family = gaussian,
)
# Chunk 1
# working directory
setwd("~/BayesWorkflow/code_r")
# packages
pacman::p_load(tidyverse,
brms,
modelr,
tidybayes)
# load functions from fun_models.R
source("fun_models.R")
source("fun_helper.R")
# Chunk 2
# sampled models
m_pooled_posterior <- readRDS("../models_R/m_multilevel_generic_fit.rds")
# Chunk 3
# read test data
train <- read_csv("../data/train.csv") %>%
mutate(idx = as_factor(idx))
plot(m_pooled_posterior,
N = 10)
mcmc_plot(m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
fixed = T) +
ggtitle("title")
?mcmc_plot
mcmc_intervals(m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
fixed = T) +
ggtitle("title")
library(bayesplot)
mcmc_intervals(m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
fixed = T) +
ggtitle("title")
library(bayesplot)
mcmc_intervals(m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma")) +
ggtitle("title")
mcmc_areas(
m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean"
)
mcmc_areas(
m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean"
) +
ggtitle("test")
mcmc_areas(
m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean"
) +
ggtitle("R/brms: Ridgeplot 80% HDI")
knit_with_parameters('~/BayesWorkflow/code_R/run_predictions.Rmd')
library(knitr)
purl("run_pooled.Rmd", output = "run_pooled.R", documentation = 2)
?stat_lineribbon
??stat_lineribbon
??stat_lineribbon
### packages & reproducibility ###
pacman::p_load(tidyverse,
brms,
modelr,
tidybayes,
bayesplot)
RANDOM_SEED = 42
### preprocessing ###
train <- read_csv("../data/train.csv") %>%
mutate(idx = as_factor(idx))
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors (could use get_prior() if in doubt).
sigma = 0.5
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### packages & reproducibility ###
pacman::p_load(tidyverse,
brms,
modelr,
tidybayes,
bayesplot)
RANDOM_SEED = 42
### preprocessing ###
train <- read_csv("../data/train.csv") %>%
mutate(idx = as_factor(idx))
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
sigma <- 0.5 # change this to reproduce other settings.
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
sigma <- 0.5 # change this to reproduce other settings.
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
sigma <- 0.5 # change this to reproduce other settings.
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
seed = RANDOM_SEED
)
pp_check(m_pooled,
nsamples = 100) +
labs(title = "R/brms: prior predictive check")
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
sigma <- 0.5 # change this to reproduce other settings.
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
prior_pooled <- c(
prior(normal(0, 0.5), class = b),
prior(normal(1.5, 0.5), class = Intercept),
prior(normal(0, 0.5), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### Prior predictive checks ###
pp_check(m_pooled,
nsamples = 100) +
labs(title = "R/brms: prior predictive check")
### sample posterior ###
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = TRUE, # only difference.
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### plot trace ###
plot(m_pooled)
### get summary (not in presentation) ###
summary(m_pooled)
### Posterior predictive checks ###
pp_check(m_pooled,
nsamples = 100) +
labs(title = "R/brms: posterior predictive check")
### Plot HDI (fixed effects) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_fitted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .value),
.width = c(.95, .8), # HDI intervals
color = "#08519C",
point_interval = mode_hdi) + ##??
geom_jitter(data = data,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (fixed)")
### Plot HDI (fixed effects) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_fitted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .value),
.width = c(.95, .8), # HDI intervals
color = "#08519C",
point_interval = mode_hdi) + ##??
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (fixed)")
### Plot HDI (full uncertainty) ###
data %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = mode_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### Plot HDI (full uncertainty) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = mode_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### Plot HDI (full uncertainty) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = median_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### Plot HDI (fixed effects) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_fitted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .value),
.width = c(.95, .8), # HDI intervals
color = "#08519C",
point_interval = median_hdi) + ##??
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (fixed)")
### Plot HDI (full uncertainty) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = median_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### HDI for parameters ###
mcmc_areas(
m_pooled,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean") +
ggtitle("R/brms: HDI intervals for parameters")
### Plot HDI (full uncertainty) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = median_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
