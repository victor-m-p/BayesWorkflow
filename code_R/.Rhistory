fixed = T) +
ggtitle("title")
?mcmc_plot
mcmc_intervals(m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
fixed = T) +
ggtitle("title")
library(bayesplot)
mcmc_intervals(m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
fixed = T) +
ggtitle("title")
library(bayesplot)
mcmc_intervals(m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma")) +
ggtitle("title")
mcmc_areas(
m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean"
)
mcmc_areas(
m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean"
) +
ggtitle("test")
mcmc_areas(
m_pooled_posterior,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean"
) +
ggtitle("R/brms: Ridgeplot 80% HDI")
knit_with_parameters('~/BayesWorkflow/code_R/run_predictions.Rmd')
library(knitr)
purl("run_pooled.Rmd", output = "run_pooled.R", documentation = 2)
?stat_lineribbon
??stat_lineribbon
??stat_lineribbon
### packages & reproducibility ###
pacman::p_load(tidyverse,
brms,
modelr,
tidybayes,
bayesplot)
RANDOM_SEED = 42
### preprocessing ###
train <- read_csv("../data/train.csv") %>%
mutate(idx = as_factor(idx))
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors (could use get_prior() if in doubt).
sigma = 0.5
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### packages & reproducibility ###
pacman::p_load(tidyverse,
brms,
modelr,
tidybayes,
bayesplot)
RANDOM_SEED = 42
### preprocessing ###
train <- read_csv("../data/train.csv") %>%
mutate(idx = as_factor(idx))
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
sigma <- 0.5 # change this to reproduce other settings.
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
sigma <- 0.5 # change this to reproduce other settings.
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
sigma <- 0.5 # change this to reproduce other settings.
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
seed = RANDOM_SEED
)
pp_check(m_pooled,
nsamples = 100) +
labs(title = "R/brms: prior predictive check")
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
sigma <- 0.5 # change this to reproduce other settings.
prior_pooled <- c(
prior(normal(0, sigma), class = b),
prior(normal(1.5, sigma), class = Intercept),
prior(normal(0, sigma), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
prior_pooled <- c(
prior(normal(0, 0.5), class = b),
prior(normal(1.5, 0.5), class = Intercept),
prior(normal(0, 0.5), class = sigma)
)
# compile model & sample prior
# NB: the tuning might be an overkill for prior sampling.
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### Prior predictive checks ###
pp_check(m_pooled,
nsamples = 100) +
labs(title = "R/brms: prior predictive check")
### sample posterior ###
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = TRUE, # only difference.
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED
)
### plot trace ###
plot(m_pooled)
### get summary (not in presentation) ###
summary(m_pooled)
### Posterior predictive checks ###
pp_check(m_pooled,
nsamples = 100) +
labs(title = "R/brms: posterior predictive check")
### Plot HDI (fixed effects) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_fitted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .value),
.width = c(.95, .8), # HDI intervals
color = "#08519C",
point_interval = mode_hdi) + ##??
geom_jitter(data = data,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (fixed)")
### Plot HDI (fixed effects) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_fitted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .value),
.width = c(.95, .8), # HDI intervals
color = "#08519C",
point_interval = mode_hdi) + ##??
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (fixed)")
### Plot HDI (full uncertainty) ###
data %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = mode_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### Plot HDI (full uncertainty) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = mode_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### Plot HDI (full uncertainty) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = median_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### Plot HDI (fixed effects) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_fitted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .value),
.width = c(.95, .8), # HDI intervals
color = "#08519C",
point_interval = median_hdi) + ##??
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (fixed)")
### Plot HDI (full uncertainty) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = median_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### HDI for parameters ###
mcmc_areas(
m_pooled,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean") +
ggtitle("R/brms: HDI intervals for parameters")
### Plot HDI (full uncertainty) ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8),
color = "#08519C",
point_interval = median_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### R: packages & reproducibility ###
pacman::p_load(
tidyverse,
brms,
modelr,
tidybayes,
bayesplot)
RANDOM_SEED = 42
### R: preprocessing ###
train <- read_csv("../data/train.csv") %>%
mutate(idx = as_factor(idx))
### R: specify model & compile ###
# formula
f_pooled <- bf(y ~ 1 + t) # complete pooling
# set priors --> can use get_prior() if in doubt.
prior_pooled <- c(
prior(normal(0, 0.5), class = b),
prior(normal(1.5, 0.5), class = Intercept),
prior(normal(0, 0.5), class = sigma))
# compile model & sample prior
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = "only",
backend = "cmdstanr")
### R: Prior predictive checks ###
pp_check(m_pooled,
nsamples = 100) +
labs(title = "R/brms: prior predictive check")
### R: sample posterior ###
m_pooled <- brm(
formula = f_pooled,
family = gaussian,
data = train,
prior = prior_pooled,
sample_prior = TRUE, # only difference.
backend = "cmdstanr",
chains = 2,
cores = 4,
iter = 4000,
warmup = 2000,
threads = threading(2), # not sure this can be done in pyMC3
control = list(adapt_delta = .99,
max_treedepth = 20),
seed = RANDOM_SEED)
### R: plot trace ###
plot(m_pooled)
### R: get summary (not displayed) ###
summary(m_pooled)
### R: Posterior predictive checks ###
pp_check(m_pooled,
nsamples = 100) +
labs(title = "R/brms: posterior predictive check")
### R: HDI prediction intervals ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_fitted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .value),
.width = c(.95, .8), # HDI intervals
color = "#08519C",
point_interval = median_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (fixed)")
### R: HDI prediction intervals ###
train %>%
data_grid(t = seq_range(t, n = 100)) %>%
add_predicted_draws(m_pooled) %>%
ggplot(aes(x = t, y = y)) +
stat_lineribbon(aes(y = .prediction),
.width = c(.95, .8), # HDI intervals
color = "#08519C",
point_interval = median_hdi) +
geom_jitter(data = train,
color = "navyblue",
shape = 1,
alpha = 0.5,
size = 2,
width = 0.1) +
scale_fill_brewer() +
ggtitle("R/brms: Prediction intervals (full)")
### R: HDI for parameters ###
mcmc_areas(
m_pooled,
pars = c("b_Intercept",
"b_t",
"sigma"),
prob = 0.8, # 80% intervals
prob_outer = 0.99, # 99%
point_est = "mean") + # or median?
ggtitle("R/brms: HDI intervals for parameters")
